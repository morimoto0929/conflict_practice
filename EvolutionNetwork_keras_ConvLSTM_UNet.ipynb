{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## mount & install"
      ],
      "metadata": {
        "id": "28u3cbZHX34s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZDmOeGeWOi0",
        "outputId": "96d07c90-7107-4290-8de3-caa1e61f2ca3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vh3WoTEzLPzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0ee8cb-dae2-495b-91ce-101b506c6771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: blosc2 in /usr/local/lib/python3.12/dist-packages (3.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (5.29.5)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.12/dist-packages (from blosc2) (1.10.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from blosc2) (1.1.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from blosc2) (4.3.8)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from blosc2) (2.11.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from blosc2) (9.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from blosc2) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2) (2025.8.3)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.4\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=03be28d30c1101acb9930c6f6beeda6266e2685be665c99b068d83a41618dc2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/21/0c/c26e09dff860a9071683e279445262346e008a9a1d2142c4ad\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.40.19-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.41.0,>=1.40.19 (from boto3)\n",
            "  Downloading botocore-1.40.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.19->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.19->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.19->boto3) (1.17.0)\n",
            "Downloading boto3-1.40.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.19-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.40.19 botocore-1.40.19 jmespath-1.0.1 s3transfer-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX blosc2\n",
        "!pip install ffmpeg\n",
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各ライブラリのインポート"
      ],
      "metadata": {
        "id": "YoOAAy87mOcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers\n",
        "import huggingface_hub\n",
        "import blosc2\n",
        "import tensorboardX\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import random\n",
        "from typing import List, Tuple, Optional\n",
        "import gc\n",
        "import boto3"
      ],
      "metadata": {
        "id": "QGj8mwWWmKss"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの読み込み"
      ],
      "metadata": {
        "id": "ZWfYyJGboLgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用例とパラメータ設定\n",
        "def setup_multi_day_training(base_date: str, num_days: int):\n",
        "    \"\"\"\n",
        "    連続する複数日のトレーニングデータを設定\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_date : str\n",
        "        開始日付 (例: \"20240809\")\n",
        "    num_days : int\n",
        "        連続する日数\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    List[str]: 日付のリスト\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    # 基準日をdatetimeオブジェクトに変換\n",
        "    base_dt = datetime.strptime(base_date, \"%Y%m%d\")\n",
        "\n",
        "    # 連続する日付のリストを生成\n",
        "    date_list = []\n",
        "    for i in range(num_days):\n",
        "        current_date = base_dt + timedelta(days=i)\n",
        "        date_list.append(current_date.strftime(\"%Y%m%d\"))\n",
        "\n",
        "    return date_list\n",
        "\n",
        "\n",
        "# 使用例: 2024年8月9日から5日間のデータを使用\n",
        "train_dates = setup_multi_day_training(\"20240809\", num_days=5)\n",
        "print(f\"Training dates: {train_dates}\")\n",
        "\n",
        "val_dates = setup_multi_day_training(\"20240819\", num_days=2)\n",
        "print(f\"Validation dates: {val_dates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqVCCZPMnNux",
        "outputId": "612d917a-fed1-4bbc-9a2f-070ce4914fdc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dates: ['20240809', '20240810', '20240811', '20240812', '20240813']\n",
            "Validation dates: ['20240819', '20240820']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lon = np.load('/content/drive/MyDrive/climate_data_platform/xrain/data/metadata/lon_centers.npy')\n",
        "print(lon.shape)\n",
        "lat = np.load('/content/drive/MyDrive/climate_data_platform/xrain/data/metadata/lat_centers.npy')\n",
        "print(lat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMR-mF1Lodm6",
        "outputId": "8d73a859-3273-448d-9811-ac25b29369bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3200, 3200)\n",
            "(3200, 3200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## datasetの処理"
      ],
      "metadata": {
        "id": "CkK4Ed7MoZlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lon_min, lon_max = 138, 140\n",
        "lat_min, lat_max = 35,  37\n",
        "\n",
        "lat_axis = lat[:, 0]         # shape (3200,)\n",
        "lon_axis = lon[0, :]         # shape (3200,)\n",
        "\n",
        "idx_lat = np.where((lat_axis >= lat_min) & (lat_axis <= lat_max))[0]\n",
        "idx_lon = np.where((lon_axis >= lon_min) & (lon_axis <= lon_max))[0]\n",
        "\n",
        "LAT_SLICE = slice(idx_lat.min(), idx_lat.max() + 1)   # 行 = 緯度\n",
        "LON_SLICE = slice(idx_lon.min(), idx_lon.max() + 1)"
      ],
      "metadata": {
        "id": "ubpoWsGtnrTm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN      = 6\n",
        "DOWNSAMPLE   = 5\n",
        "SCALE_FACTOR = 300.0      # mm h‑1 の上限\n",
        "BATCH_SIZE   = 16         # ConvLSTM はメモリ使用量大 (Reduced from 16 to 8)\n",
        "EPOCHS       = 200\n",
        "\n",
        "# 縮小後の空間サイズを自動算出\n",
        "H = math.ceil((LAT_SLICE.stop - LAT_SLICE.start) / DOWNSAMPLE)  # 緯度方向\n",
        "W = math.ceil((LON_SLICE.stop - LON_SLICE.start) / DOWNSAMPLE)  # 経度方向\n",
        "print(f\"Input size  : (T={SEQ_LEN}, H={H}, W={W})\")\n",
        "\n",
        "estimated_frames_per_day = 1440\n",
        "total_frames = len(train_dates) * estimated_frames_per_day\n",
        "total_val_frames = len(val_dates) * estimated_frames_per_day\n",
        "multi_steps_per_epoch = max(1, (total_frames - SEQ_LEN) // BATCH_SIZE)\n",
        "multi_val_steps = max(1, (total_val_frames - SEQ_LEN) // BATCH_SIZE)\n",
        "\n",
        "print(f\"Estimated steps per epoch: {multi_steps_per_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwIrdd_YouVG",
        "outputId": "76208bd3-f5e7-4d4a-8a06-f6fd5669e4b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input size  : (T=6, H=192, W=128)\n",
            "Estimated steps per epoch: 449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 論文に忠実なImportance Sampling実装\n",
        "\n",
        "def calculate_acceptance_probability(crop: np.ndarray,\n",
        "                                   training_mode: bool = True,\n",
        "                                   epsilon: float = 1e-6) -> float:\n",
        "    \"\"\"\n",
        "    降水量データの受容確率を計算する（既存関数を置き換え）\n",
        "    \"\"\"\n",
        "    valid_mask = ~np.isnan(crop)\n",
        "\n",
        "    if training_mode:\n",
        "        # Training: g(x) = 1 - e^(-x) for valid, g(x) = 0 for missing\n",
        "        valid_values = crop[valid_mask]\n",
        "        g_values = 1.0 - np.exp(-valid_values)\n",
        "        total_g = np.sum(g_values)  # missing pixelsは自動的に0\n",
        "    else:\n",
        "        # Test/Validation: g(x) = x for all (論文に従い、missing valueは0として扱う)\n",
        "        safe_crop = np.nan_to_num(crop, nan=0.0)\n",
        "        total_g = np.sum(safe_crop)\n",
        "\n",
        "    return total_g + epsilon\n",
        "\n",
        "\n",
        "def hierarchical_sampling(crops_with_probs: List[Tuple[np.ndarray, float]],\n",
        "                        n_samples: int,\n",
        "                        remove_threshold: bool = False) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    階層的サンプリング（閾値による排除を撤廃）\n",
        "    \"\"\"\n",
        "    if not crops_with_probs:\n",
        "        return []\n",
        "\n",
        "    # 論文では閾値による排除は行わない - 全てのクロップが候補\n",
        "    all_crops = crops_with_probs\n",
        "\n",
        "    if not all_crops:\n",
        "        return []\n",
        "\n",
        "    # 受容確率を正規化\n",
        "    probs = np.array([prob for _, prob in all_crops])\n",
        "\n",
        "    # ゼロ確率を避けるための最小値設定\n",
        "    min_prob = np.min(probs[probs > 0]) * 0.001 if np.any(probs > 0) else epsilon\n",
        "    probs = np.maximum(probs, min_prob)\n",
        "\n",
        "    probs = probs / np.sum(probs)  # 正規化\n",
        "\n",
        "    # 論文に従った確率的サンプリング（with replacement）\n",
        "    selected_indices = np.random.choice(\n",
        "        len(all_crops),\n",
        "        size=min(n_samples, len(all_crops)),\n",
        "        p=probs,\n",
        "        replace=True  # 重要: 重複選択を許可\n",
        "    )\n",
        "\n",
        "    selected_crops = [all_crops[i][0] for i in selected_indices]\n",
        "    return selected_crops\n",
        "\n",
        "\n",
        "def calculate_importance_sampling_steps(date_list: List[str],\n",
        "                                      batch_size: int,\n",
        "                                      training_mode: bool = True) -> int:\n",
        "    \"\"\"\n",
        "    Importance Samplingでのsteps per epochを計算（既存関数を置き換え）\n",
        "    \"\"\"\n",
        "\n",
        "    # 論文の設定に基づく推定\n",
        "    estimated_frames_per_day = 144  # 10分間隔で1日144フレーム\n",
        "    total_frames = len(date_list) * estimated_frames_per_day\n",
        "\n",
        "    if training_mode:\n",
        "        # Training: 256x256クロップ、stride=32\n",
        "        crop_size = (256, 256)\n",
        "        stride = 32\n",
        "        # 推定空間サイズ（ダウンサンプリング後）\n",
        "        estimated_spatial_size = (400, 300)\n",
        "    else:\n",
        "        # Test/Validation: 512x512クロップ\n",
        "        crop_size = (512, 512)\n",
        "        stride = 32\n",
        "        estimated_spatial_size = (400, 300)\n",
        "\n",
        "    # 空間方向のクロップ数\n",
        "    h_crops = max(1, (estimated_spatial_size[0] - crop_size[0]) // stride + 1)\n",
        "    w_crops = max(1, (estimated_spatial_size[1] - crop_size[1]) // stride + 1)\n",
        "    spatial_crops_per_frame = h_crops * w_crops\n",
        "\n",
        "    # 時間方向の有効フレーム数（シーケンス長を考慮）\n",
        "    seq_len = 6\n",
        "    valid_temporal_frames = max(1, total_frames - seq_len - 1)\n",
        "\n",
        "    # 総クロップ数\n",
        "    total_possible_crops = valid_temporal_frames * spatial_crops_per_frame\n",
        "\n",
        "    # 論文に従い、importance samplingによる実効的な増加は控えめに\n",
        "    if training_mode:\n",
        "        # Training: 重要なサンプルの重複選択により若干増加\n",
        "        effective_factor = 1.2\n",
        "    else:\n",
        "        # Validation: ほぼ実データ通り\n",
        "        effective_factor = 1.0\n",
        "\n",
        "    effective_crops = int(total_possible_crops * effective_factor)\n",
        "    steps_per_epoch = max(1, effective_crops // batch_size)\n",
        "\n",
        "    print(f\"Steps calculation ({'train' if training_mode else 'val'}):\")\n",
        "    print(f\"  Days: {len(date_list)}\")\n",
        "    print(f\"  Frames per day: {estimated_frames_per_day}\")\n",
        "    print(f\"  Spatial crops per frame: {spatial_crops_per_frame}\")\n",
        "    print(f\"  Total possible crops: {total_possible_crops}\")\n",
        "    print(f\"  Effective factor: {effective_factor}\")\n",
        "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "    return steps_per_epoch\n",
        "\n",
        "\n",
        "def importance_sampling_generator(date_list: List[str],\n",
        "                                batch_size: int,\n",
        "                                lon_slice: slice,\n",
        "                                lat_slice: slice,\n",
        "                                seq_len: int = 6,\n",
        "                                downsample: int = 5,\n",
        "                                training_mode: bool = True,\n",
        "                                buffer_size: int = 2):\n",
        "    \"\"\"\n",
        "    Importance Samplingを適用したバッチジェネレータ（既存関数を置き換え）\n",
        "    \"\"\"\n",
        "\n",
        "    # Wasabi Cloud設定（既存コードと同じ）\n",
        "    from google.colab import userdata\n",
        "    import boto3\n",
        "    import blosc2\n",
        "    import os\n",
        "    import gc\n",
        "\n",
        "    WASABI_ENDPOINT = \"https://s3.ap-northeast-1.wasabisys.com\"\n",
        "    WASABI_ACCESS_KEY = userdata.get('WASABI_ACCESS_KEY')\n",
        "    WASABI_SECRET_KEY = userdata.get('WASABI_SECRET_KEY')\n",
        "    BUCKET_NAME = \"xrain-composite-cx\"\n",
        "\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        endpoint_url=WASABI_ENDPOINT,\n",
        "        aws_access_key_id=WASABI_ACCESS_KEY,\n",
        "        aws_secret_access_key=WASABI_SECRET_KEY,\n",
        "        region_name='ap-northeast-1'\n",
        "    )\n",
        "\n",
        "    def load_data_for_date(date: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"データロード（既存と同じ）\"\"\"\n",
        "        base_path = \"b2nd\"\n",
        "        year = date[0:4]\n",
        "        month = date[4:6]\n",
        "        filename = f\"{base_path}/{year}/{month}/cx-{date}.b2nd\"\n",
        "        temp_filename = f\"/tmp/cx-{date}.b2nd\"\n",
        "\n",
        "        try:\n",
        "            s3_client.download_file(BUCKET_NAME, filename, temp_filename)\n",
        "            rains = blosc2.open(temp_filename)\n",
        "            data = rains[:, lat_slice, lon_slice]\n",
        "            data = data[:, ::downsample, ::downsample]\n",
        "            if os.path.exists(temp_filename):\n",
        "                os.remove(temp_filename)\n",
        "            return data.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load data for {date}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_crops_with_probabilities(data: np.ndarray) -> List[Tuple[np.ndarray, float]]:\n",
        "        \"\"\"クロップ生成と重要度計算\"\"\"\n",
        "        T, H, W = data.shape\n",
        "\n",
        "        # 論文に従ったクロップサイズ\n",
        "        if training_mode:\n",
        "            crop_h, crop_w = 256, 256\n",
        "        else:\n",
        "            crop_h, crop_w = 256, 256  # Validationも256x256で統一（メモリ節約）\n",
        "\n",
        "        stride = 32  # 論文の設定\n",
        "        crops_with_probs = []\n",
        "\n",
        "        # 時間方向のクロップ（270分 = 27フレーム@10分間隔）\n",
        "        temporal_crop_size = 27\n",
        "\n",
        "        for t_start in range(0, max(1, T - temporal_crop_size), temporal_crop_size // 2):\n",
        "            t_end = min(t_start + temporal_crop_size, T)\n",
        "\n",
        "            # 空間方向のクロップ\n",
        "            for h_start in range(0, max(1, H - crop_h + 1), stride):\n",
        "                for w_start in range(0, max(1, W - crop_w + 1), stride):\n",
        "                    h_end = min(h_start + crop_h, H)\n",
        "                    w_end = min(w_start + crop_w, W)\n",
        "\n",
        "                    crop = data[t_start:t_end, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                    # 受容確率を計算\n",
        "                    prob = calculate_acceptance_probability(crop, training_mode)\n",
        "                    crops_with_probs.append((crop, prob))\n",
        "\n",
        "        return crops_with_probs\n",
        "\n",
        "    # データバッファ\n",
        "    data_buffer = []\n",
        "    current_buffer_dates = []\n",
        "\n",
        "    while True:\n",
        "        for start_date_idx in range(len(date_list)):\n",
        "            buffer_dates = date_list[start_date_idx:start_date_idx + buffer_size]\n",
        "\n",
        "            if current_buffer_dates != buffer_dates:\n",
        "                data_buffer = []\n",
        "                gc.collect()\n",
        "\n",
        "                for date in buffer_dates:\n",
        "                    data = load_data_for_date(date)\n",
        "                    if data is not None:\n",
        "                        data_buffer.append(data)\n",
        "\n",
        "                current_buffer_dates = buffer_dates.copy()\n",
        "\n",
        "                if not data_buffer:\n",
        "                    continue\n",
        "\n",
        "                combined_data = np.concatenate(data_buffer, axis=0)\n",
        "                print(f\"Loaded data for dates {buffer_dates}, combined shape: {combined_data.shape}\")\n",
        "\n",
        "            # クロップ生成\n",
        "            crops_with_probs = generate_crops_with_probabilities(combined_data)\n",
        "\n",
        "            if not crops_with_probs:\n",
        "                continue\n",
        "\n",
        "            # 階層的サンプリング（閾値なし）\n",
        "            selected_crops = hierarchical_sampling(\n",
        "                crops_with_probs,\n",
        "                batch_size * 5  # 充分なサンプル数を確保\n",
        "            )\n",
        "\n",
        "            # シーケンス生成\n",
        "            random.shuffle(selected_crops)\n",
        "\n",
        "            xs, ys = [], []\n",
        "            for crop in selected_crops:\n",
        "                if len(xs) >= batch_size:\n",
        "                    break\n",
        "\n",
        "                T_crop = crop.shape[0]\n",
        "                if T_crop <= seq_len:\n",
        "                    continue\n",
        "\n",
        "                max_start = T_crop - seq_len - 1\n",
        "                t_start = np.random.randint(0, max(1, max_start))\n",
        "\n",
        "                window = crop[t_start:t_start + seq_len + 1]\n",
        "\n",
        "                # NaN処理を改善\n",
        "                if np.isnan(window).any():\n",
        "                    window = np.nan_to_num(window, nan=0.0)\n",
        "\n",
        "                x_seq = window[:-1][..., None]\n",
        "                y_target = window[-1][..., None]\n",
        "\n",
        "                xs.append(x_seq)\n",
        "                ys.append(y_target)\n",
        "\n",
        "            if xs:\n",
        "                yield np.array(xs, dtype=np.float32), np.array(ys, dtype=np.float32)"
      ],
      "metadata": {
        "id": "X0IsufHGVBG2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dates = setup_multi_day_training(\"20240809\", num_days=2)\n",
        "val_dates = setup_multi_day_training(\"20240819\", num_days=2)\n",
        "\n",
        "train_gen = importance_sampling_generator(\n",
        "    train_dates, BATCH_SIZE, LON_SLICE, LAT_SLICE, SEQ_LEN, DOWNSAMPLE,\n",
        "    training_mode=True\n",
        ")\n",
        "\n",
        "val_gen = importance_sampling_generator(\n",
        "    val_dates, BATCH_SIZE, LON_SLICE, LAT_SLICE, SEQ_LEN, DOWNSAMPLE,\n",
        "    training_mode=False\n",
        ")\n",
        "\n",
        "# Steps計算\n",
        "train_steps = calculate_importance_sampling_steps(train_dates, BATCH_SIZE, training_mode=True)\n",
        "val_steps = calculate_importance_sampling_steps(val_dates, BATCH_SIZE, training_mode=False)\n",
        "\n",
        "# 統計分析\n",
        "# analyze_importance_sampling_statistics(\n",
        "#     train_dates, LON_SLICE, LAT_SLICE, DOWNSAMPLE\n",
        "# )"
      ],
      "metadata": {
        "id": "hUsyeOtRi6i1",
        "outputId": "871572ac-0f69-48c8-a8c9-707d95ccd41c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps calculation (train):\n",
            "  Days: 2\n",
            "  Frames per day: 144\n",
            "  Spatial crops per frame: 10\n",
            "  Total possible crops: 2810\n",
            "  Effective factor: 1.2\n",
            "  Steps per epoch: 210\n",
            "Steps calculation (val):\n",
            "  Days: 2\n",
            "  Frames per day: 144\n",
            "  Spatial crops per frame: 1\n",
            "  Total possible crops: 281\n",
            "  Effective factor: 1.0\n",
            "  Steps per epoch: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの定義"
      ],
      "metadata": {
        "id": "TBKg9fXhU-4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bilinear ワープ & Sobel\n",
        "def dense_image_warp_bilinear(img, flow):\n",
        "    \"\"\"\n",
        "    img:  (B,H,W,C), flow: (B,H,W,2) with (dy, dx) in pixels (backward sampling)\n",
        "    returns: warped image (B,H,W,C)\n",
        "    \"\"\"\n",
        "    B = tf.shape(img)[0]\n",
        "    H = tf.shape(img)[1]\n",
        "    W = tf.shape(img)[2]\n",
        "    C = tf.shape(img)[3]\n",
        "\n",
        "    # base grid\n",
        "    y = tf.cast(tf.range(H), tf.float32)\n",
        "    x = tf.cast(tf.range(W), tf.float32)\n",
        "    Y, X = tf.meshgrid(y, x, indexing=\"ij\")           # (H,W)\n",
        "    grid = tf.stack([Y, X], axis=-1)                 # (H,W,2)\n",
        "    grid = tf.broadcast_to(grid[None, ...], [B, H, W, 2])\n",
        "\n",
        "    # sample coords\n",
        "    coords = grid + tf.cast(flow, tf.float32)        # (B,H,W,2)\n",
        "    y_s = coords[..., 0]\n",
        "    x_s = coords[..., 1]\n",
        "\n",
        "    # neighbors\n",
        "    y0 = tf.floor(y_s);  x0 = tf.floor(x_s)\n",
        "    y1 = y0 + 1.0;       x1 = x0 + 1.0\n",
        "\n",
        "    y0c = tf.clip_by_value(tf.cast(y0, tf.int32), 0, H-1)\n",
        "    y1c = tf.clip_by_value(tf.cast(y1, tf.int32), 0, H-1)\n",
        "    x0c = tf.clip_by_value(tf.cast(x0, tf.int32), 0, W-1)\n",
        "    x1c = tf.clip_by_value(tf.cast(x1, tf.int32), 0, W-1)\n",
        "\n",
        "    # gather 4 neighbors\n",
        "    batch_idx = tf.reshape(tf.range(B, dtype=tf.int32), [B,1,1])\n",
        "    batch_idx = tf.tile(batch_idx, [1,H,W])\n",
        "\n",
        "    def gather(b, yy, xx):\n",
        "        idx = tf.stack([b, yy, xx], axis=-1)     # (B,H,W,3)\n",
        "        return tf.gather_nd(img, idx)            # (B,H,W,C)\n",
        "\n",
        "    Ia = gather(batch_idx, y0c, x0c)\n",
        "    Ib = gather(batch_idx, y0c, x1c)\n",
        "    Ic = gather(batch_idx, y1c, x0c)\n",
        "    Id = gather(batch_idx, y1c, x1c)\n",
        "\n",
        "    # bilinear weights\n",
        "    wy1 = y1 - y_s\n",
        "    wx1 = x1 - x_s\n",
        "    wy0 = 1.0 - wy1\n",
        "    wx0 = 1.0 - wx1\n",
        "\n",
        "    wa = (wy1 * wx1)[..., None]\n",
        "    wb = (wy1 * wx0)[..., None]\n",
        "    wc = (wy0 * wx1)[..., None]\n",
        "    wd = (wy0 * wx0)[..., None]\n",
        "\n",
        "    out = wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "    return out\n",
        "\n",
        "\n",
        "def sobel_grad_l2(img):\n",
        "    \"\"\"\n",
        "    img: (B,H,W,1)\n",
        "    returns: (B,H,W,1) with sqrt(gx^2 + gy^2)\n",
        "    \"\"\"\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    kx = tf.constant([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=tf.float32)\n",
        "    ky = tf.constant([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=tf.float32)\n",
        "    kx = tf.reshape(kx, [3,3,1,1])\n",
        "    ky = tf.reshape(ky, [3,3,1,1])\n",
        "    gx = tf.nn.conv2d(img, kx, strides=1, padding=\"SAME\")\n",
        "    gy = tf.nn.conv2d(img, ky, strides=1, padding=\"SAME\")\n",
        "    return tf.sqrt(tf.maximum(gx*gx + gy*gy, 1e-12))"
      ],
      "metadata": {
        "id": "YDhDKXwk6WTg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvolutionUNetConvLSTM(keras.Model):\n",
        "    \"\"\"\n",
        "    inputs:  (B, T, H, W, 1)   mm/h\n",
        "    outputs: (B, H, W, 1)      mm/h  （x_evo: 1ステップ先）\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, scale_factor=300.0, max_disp=3.0, lambda_motion=1e-2):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.scale = float(scale_factor)\n",
        "        self.max_disp = float(max_disp)\n",
        "        self.lambda_motion = float(lambda_motion)\n",
        "\n",
        "        Act = \"relu\"\n",
        "        # ---- Encoder ----\n",
        "        self.norm_in = layers.Rescaling(1.0 / self.scale)\n",
        "        self.c1a = layers.ConvLSTM2D(32, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.c1b = layers.ConvLSTM2D(32, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.p1  = layers.MaxPooling3D(pool_size=(1,2,2), padding=\"same\")\n",
        "\n",
        "        self.c2a = layers.ConvLSTM2D(64, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.c2b = layers.ConvLSTM2D(64, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.p2  = layers.MaxPooling3D(pool_size=(1,2,2), padding=\"same\")\n",
        "\n",
        "        # ---- Bottleneck ----\n",
        "        self.c3a = layers.ConvLSTM2D(128, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.c3b = layers.ConvLSTM2D(128, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "\n",
        "        # ---- Decoder ----\n",
        "        self.u4  = layers.UpSampling3D(size=(1,2,2))\n",
        "        self.c4a = layers.ConvLSTM2D(64, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.c4b = layers.ConvLSTM2D(64, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "\n",
        "        self.u5  = layers.UpSampling3D(size=(1,2,2))\n",
        "        self.c5a = layers.ConvLSTM2D(32, 3, padding=\"same\", return_sequences=True, activation=Act)\n",
        "        self.c5b = layers.ConvLSTM2D(32, 3, padding=\"same\", return_sequences=False, activation=Act)\n",
        "\n",
        "        # ---- Heads ----\n",
        "        self.head_flow = layers.Conv2D(2, 3, padding=\"same\", activation=\"tanh\")  # -> [-1,1]\n",
        "        self.head_res  = layers.Conv2D(1, 1, padding=\"same\", activation=None)\n",
        "        self.denorm_out = layers.Rescaling(self.scale)\n",
        "\n",
        "        # metrics\n",
        "        self.loss_tracker      = keras.metrics.Mean(name=\"loss\")\n",
        "        self.accum_tracker     = keras.metrics.Mean(name=\"accum_loss\")\n",
        "        self.motion_tracker    = keras.metrics.Mean(name=\"motion_reg\")\n",
        "        self.mae_tracker       = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
        "\n",
        "    def evolve_step(self, x_prev_norm, flow, resid):\n",
        "        # flow: [-1,1] → [-max_disp, max_disp]\n",
        "        disp = tf.cast(flow, x_prev_norm.dtype) * self.max_disp\n",
        "        x_adv_norm = dense_image_warp_bilinear(x_prev_norm, disp)  # (B,H,W,1)\n",
        "        x_evo_norm = tf.nn.relu(x_adv_norm + resid)\n",
        "        return x_adv_norm, x_evo_norm\n",
        "\n",
        "    def _forward_heads(self, x):\n",
        "        z = self.norm_in(x)\n",
        "        c1 = self.c1b(self.c1a(z))\n",
        "        p1 = self.p1(c1)\n",
        "\n",
        "        c2 = self.c2b(self.c2a(p1))\n",
        "        p2 = self.p2(c2)\n",
        "\n",
        "        c3 = self.c3b(self.c3a(p2))\n",
        "\n",
        "        u4 = self.u4(c3); u4 = layers.concatenate([u4, c2])\n",
        "        c4 = self.c4b(self.c4a(u4))\n",
        "\n",
        "        u5 = self.u5(c4); u5 = layers.concatenate([u5, c1])\n",
        "        c5 = self.c5a(u5)\n",
        "        feat = self.c5b(c5)                     # (B,H,W,32)\n",
        "\n",
        "        flow = self.head_flow(feat)             # [-1,1]\n",
        "        res  = self.head_res(feat)              # normalized residual\n",
        "        return flow, res\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        flow, res = self._forward_heads(inputs)\n",
        "        x_prev_norm = self.norm_in(inputs[:, -1, ...])\n",
        "        _, x_evo_norm = self.evolve_step(x_prev_norm, flow, res)\n",
        "        return self.denorm_out(x_evo_norm)      # (B,H,W,1) mm/h\n",
        "\n",
        "    def weighted_l1(self, y_true, y_pred):\n",
        "        w = tf.minimum(24.0, 1.0 + y_true)\n",
        "        return tf.reduce_mean(tf.abs(y_true - y_pred) * w)\n",
        "\n",
        "    def motion_reg(self, flow_px, weight_field_mmph):\n",
        "        # flow_px: (B,H,W,2) in pixels (after scaling)\n",
        "        vy = flow_px[..., 0:1]; vx = flow_px[..., 1:2]\n",
        "        gy = sobel_grad_l2(vy); gx = sobel_grad_l2(vx)    # (B,H,W,1)\n",
        "        w  = tf.minimum(24.0, 1.0 + weight_field_mmph)\n",
        "        return tf.reduce_mean(w * (gy + gx))\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data  # x:(B,T,H,W,1) mm/h, y:(B,H,W,1) mm/h\n",
        "        with tf.GradientTape() as tape:\n",
        "            flow, res = self._forward_heads(x)\n",
        "            x_prev_norm = self.norm_in(x[:, -1, ...])\n",
        "            x_adv_norm, x_evo_norm = self.evolve_step(x_prev_norm, flow, res)\n",
        "            x_adv = self.denorm_out(x_adv_norm)\n",
        "            x_evo = self.denorm_out(x_evo_norm)\n",
        "\n",
        "            accum = self.weighted_l1(y, x_adv) + self.weighted_l1(y, x_evo)\n",
        "            motion = self.motion_reg(flow * self.max_disp, y)\n",
        "            loss = accum + self.lambda_motion * motion\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.accum_tracker.update_state(accum)\n",
        "        self.motion_tracker.update_state(motion)\n",
        "        self.mae_tracker.update_state(y, x_evo)\n",
        "        return {\"loss\": self.loss_tracker.result(),\n",
        "                \"accum_loss\": self.accum_tracker.result(),\n",
        "                \"motion_reg\": self.motion_tracker.result(),\n",
        "                \"mae\": self.mae_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        flow, res = self._forward_heads(x)\n",
        "        x_prev_norm = self.norm_in(x[:, -1, ...])\n",
        "        x_adv_norm, x_evo_norm = self.evolve_step(x_prev_norm, flow, res)\n",
        "        x_adv = self.denorm_out(x_adv_norm)\n",
        "        x_evo = self.denorm_out(x_evo_norm)\n",
        "\n",
        "        accum = self.weighted_l1(y, x_adv) + self.weighted_l1(y, x_evo)\n",
        "        motion = self.motion_reg(flow * self.max_disp, y)\n",
        "        loss = accum + self.lambda_motion * motion\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.accum_tracker.update_state(accum)\n",
        "        self.motion_tracker.update_state(motion)\n",
        "        self.mae_tracker.update_state(y, x_evo)\n",
        "        return {\"loss\": self.loss_tracker.result(),\n",
        "                \"accum_loss\": self.accum_tracker.result(),\n",
        "                \"motion_reg\": self.motion_tracker.result(),\n",
        "                \"mae\": self.mae_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.accum_tracker, self.motion_tracker, self.mae_tracker]"
      ],
      "metadata": {
        "id": "Deqy0SEko7Uy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Steps per epochの再計算\n",
        "def calculate_importance_sampling_steps(date_list: List[str],\n",
        "                                      batch_size: int,\n",
        "                                      crop_size: Tuple[int, int] = (256, 256),\n",
        "                                      stride: int = 32,\n",
        "                                      downsample: int = 5,\n",
        "                                      importance_factor: float = 2.0) -> int:\n",
        "    \"\"\"\n",
        "    Importance Samplingでのsteps per epochを計算\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    importance_factor : float\n",
        "        Importance Samplingによる有効サンプル増加係数\n",
        "        重要なサンプルが繰り返し選ばれるため、実効的なデータ量が増加する\n",
        "    \"\"\"\n",
        "\n",
        "    # 推定フレーム数（既存の計算）\n",
        "    estimated_frames_per_day = 1440  # 1日 = 1440分, 10分間隔で144フレーム\n",
        "    total_frames = len(date_list) * estimated_frames_per_day\n",
        "\n",
        "    # 空間クロップ数の推定\n",
        "    # 仮定: 各日のデータサイズを (144, 400, 300) とする\n",
        "    estimated_h, estimated_w = 400, 300  # ダウンサンプリング後のサイズ\n",
        "    crop_h, crop_w = crop_size\n",
        "\n",
        "    # クロップ数の計算\n",
        "    n_crops_h = max(1, (estimated_h - crop_h) // stride + 1)\n",
        "    n_crops_w = max(1, (estimated_w - crop_w) // stride + 1)\n",
        "    crops_per_day = n_crops_h * n_crops_w\n",
        "\n",
        "    # Importance Samplingによる有効サンプル数\n",
        "    # 重要なサンプルが重複選択されるため、実効的なサンプル数は増加\n",
        "    effective_samples = int(len(date_list) * crops_per_day * importance_factor)\n",
        "\n",
        "    # バッチ数に変換\n",
        "    steps_per_epoch = max(1, effective_samples // batch_size)\n",
        "\n",
        "    print(f\"Importance Sampling Steps Calculation:\")\n",
        "    print(f\"  Days: {len(date_list)}\")\n",
        "    print(f\"  Estimated crops per day: {crops_per_day}\")\n",
        "    print(f\"  Importance factor: {importance_factor}\")\n",
        "    print(f\"  Effective samples: {effective_samples}\")\n",
        "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "    return steps_per_epoch\n",
        "\n",
        "# steps per epochを計算\n",
        "importance_steps_per_epoch = calculate_importance_sampling_steps(\n",
        "    train_dates, BATCH_SIZE, importance_factor=2.5\n",
        ")\n",
        "\n",
        "importance_val_steps = calculate_importance_sampling_steps(\n",
        "    val_dates, BATCH_SIZE, importance_factor=1.5  # 検証では重複が少ない\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 例: 既存の steps_per_epoch, EPOCHS をそのまま使う\n",
        "warmup_epochs = 5\n",
        "lr_start = 3e-4   # ウォームアップ開始\n",
        "lr_max   = 1e-3   # ピーク\n",
        "lr_min   = 1e-4   # 終了時（最小）\n",
        "\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, steps_per_epoch, total_epochs,\n",
        "                 warmup_epochs, lr_start, lr_max, lr_min):\n",
        "        self.spe = float(steps_per_epoch)\n",
        "        self.T = float(total_epochs)\n",
        "        self.warm = float(warmup_epochs)\n",
        "        self.lr_start = float(lr_start)\n",
        "        self.lr_max = float(lr_max)\n",
        "        self.lr_min = float(lr_min)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # step → epoch に変換\n",
        "        epoch = tf.cast(step, tf.float32) / self.spe\n",
        "        # 1) Warmup\n",
        "        lr_warm = self.lr_start + (self.lr_max - self.lr_start) * (epoch / self.warm)\n",
        "        # 2) Cosine decay (after warmup)\n",
        "        progress = tf.clip_by_value((epoch - self.warm) / (self.T - self.warm), 0.0, 1.0)\n",
        "        lr_cos = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1.0 + tf.cos(np.pi * progress))\n",
        "        return tf.where(epoch < self.warm, lr_warm, lr_cos)\n",
        "\n",
        "# 学習率スケジュールを作成\n",
        "lr_sched = WarmupCosine(importance_steps_per_epoch, EPOCHS, warmup_epochs, lr_start, lr_max, lr_min)\n",
        "\n",
        "# Optimizer を差し替え（clipnorm で安定化）\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr_sched, clipnorm=1.0, epsilon=1e-7)"
      ],
      "metadata": {
        "id": "TLKHLWXzW8ND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b9fdec-9041-4703-8760-256669a54eff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importance Sampling Steps Calculation:\n",
            "  Days: 2\n",
            "  Estimated crops per day: 10\n",
            "  Importance factor: 2.5\n",
            "  Effective samples: 50\n",
            "  Steps per epoch: 3\n",
            "Importance Sampling Steps Calculation:\n",
            "  Days: 2\n",
            "  Estimated crops per day: 10\n",
            "  Importance factor: 1.5\n",
            "  Effective samples: 30\n",
            "  Steps per epoch: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EvolutionUNetConvLSTM(\n",
        "    seq_len=SEQ_LEN,\n",
        "    scale_factor=SCALE_FACTOR,\n",
        "    max_disp=3.0,\n",
        "    lambda_motion=1e-2\n",
        ")\n",
        "model.compile(optimizer=opt)\n",
        "model.build(input_shape=(None, SEQ_LEN, H, W, 1))\n",
        "_ = model(tf.zeros((1, SEQ_LEN, H, W, 1), dtype=tf.float32), training=False)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "8hCkeEPbpQYe",
        "outputId": "53b717f8-5cdb-4a9c-b940-3e31edf7fc71"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'evolution_u_net_conv_lstm', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"evolution_u_net_conv_lstm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"evolution_u_net_conv_lstm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d (\u001b[38;5;33mConvLSTM2D\u001b[0m)        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m38,144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_1 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)    │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_2 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m221,440\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_3 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_4 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m885,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_5 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling3d (\u001b[38;5;33mUpSampling3D\u001b[0m)    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_6 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_7 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling3d_1 (\u001b[38;5;33mUpSampling3D\u001b[0m)  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_8 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_9 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2\u001b[0m)       │           \u001b[38;5;34m578\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │            \u001b[38;5;34m33\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)    │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,440</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">885,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">578</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,801,315\u001b[0m (14.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,801,315</span> (14.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,801,315\u001b[0m (14.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,801,315</span> (14.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習可視化用"
      ],
      "metadata": {
        "id": "ztrCRGorDwo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import tensorflow as tf\n",
        "import tensorboardX\n",
        "\n",
        "# --- JMA colormap (既存と同じ) ---\n",
        "JMA_COLORS = [\"#f2f2ff\",\"#a0d2ff\",\"#218cff\",\"#0000ff\",\n",
        "              \"#faf500\",\"#ff9900\",\"#ff2100\",\"#c800c8\"]\n",
        "JMA_BOUNDS = [0,1,5,10,20,30,50,80,1000]\n",
        "JMA_CMAP   = mcolors.ListedColormap(JMA_COLORS)\n",
        "JMA_NORM   = mcolors.BoundaryNorm(JMA_BOUNDS, JMA_CMAP.N)\n",
        "\n",
        "def _quiver_downsample(vy, vx, step=8):\n",
        "    \"\"\"矢印が多すぎて見づらいのを防ぐため、格子を間引く\"\"\"\n",
        "    return vy[::step, ::step], vx[::step, ::step]\n",
        "\n",
        "def _imshow_mmhr(ax, img2d, title=\"\"):\n",
        "    m = ax.imshow(img2d, cmap=JMA_CMAP, norm=JMA_NORM)\n",
        "    ax.set_title(title); ax.set_xticks([]); ax.set_yticks([])\n",
        "    return m"
      ],
      "metadata": {
        "id": "Ka0dWwrmDy79"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_flow_source(model, x_batch, y_batch=None, quiver_step=8, figsize=(12,6), show=False, print_stats=True):\n",
        "    \"\"\"\n",
        "    model  : EvolutionUNetConvLSTM\n",
        "    x_batch: (B,T,H,W,1)  mm/h\n",
        "    y_batch: (B,H,W,1) or None\n",
        "    \"\"\"\n",
        "    model.trainable = False\n",
        "    x_batch = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
        "    B, T, H, W, _ = x_batch.shape\n",
        "\n",
        "    # 1) ヘッド出力（flow [-1,1], res(正規化), 最後フレーム正規化）\n",
        "    flow, res_norm = model._forward_heads(x_batch)      # (B,H,W,2), (B,H,W,1)\n",
        "    x_prev_norm = model.norm_in(x_batch[:, -1, ...])    # (B,H,W,1)\n",
        "\n",
        "    # 2) 進化演算子：x_adv_norm, x_evo_norm\n",
        "    x_adv_norm, x_evo_norm = model.evolve_step(x_prev_norm, flow, res_norm)\n",
        "\n",
        "    # 3) 可視化は mm/h で\n",
        "    x_prev = model.denorm_out(x_prev_norm).numpy()[0, ..., 0]\n",
        "    x_adv  = model.denorm_out(x_adv_norm ).numpy()[0, ..., 0]\n",
        "    x_evo  = model.denorm_out(x_evo_norm ).numpy()[0, ..., 0]\n",
        "    source = model.denorm_out(res_norm    ).numpy()[0, ..., 0]  # ← 残差(mm/h)\n",
        "\n",
        "    vy = (flow.numpy()[0, ..., 0]) * model.max_disp  # [px/step]\n",
        "    vx = (flow.numpy()[0, ..., 1]) * model.max_disp\n",
        "\n",
        "    # 4) 図を描く\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax1 = fig.add_subplot(2,3,1); _imshow_mmhr(ax1, x_prev, \"x_prev (t-1)\")\n",
        "    ax2 = fig.add_subplot(2,3,2); _imshow_mmhr(ax2, x_adv,  \"x_adv = warp(x_prev)\")\n",
        "    ax3 = fig.add_subplot(2,3,3); _imshow_mmhr(ax3, x_evo,  \"x_evo = ReLU(x_adv + s)\")\n",
        "    ax4 = fig.add_subplot(2,3,4); _imshow_mmhr(ax4, source, \"source s (mm/h)\")\n",
        "    ax5 = fig.add_subplot(2,3,5)\n",
        "    _imshow_mmhr(ax5, x_prev, \"flow (quiver on x_prev)\")\n",
        "    # クイバーを間引いて描画\n",
        "    yy, xx = np.mgrid[0:H, 0:W]\n",
        "    yq, xq = yy[::quiver_step, ::quiver_step], xx[::quiver_step, ::quiver_step]\n",
        "    vyq, vxq = vy[::quiver_step, ::quiver_step], vx[::quiver_step, ::quiver_step]\n",
        "    ax5.quiver(xq, yq, vxq, vyq, angles='xy', scale_units='xy', scale=1.0, width=0.002)\n",
        "\n",
        "    # 5) もし教師があれば差分も\n",
        "    if y_batch is not None:\n",
        "        y = tf.convert_to_tensor(y_batch, dtype=tf.float32).numpy()[0, ..., 0]\n",
        "        ax6 = fig.add_subplot(2,3,6); _imshow_mmhr(ax6, np.abs(y - x_evo), \"|y - x_evo|\")\n",
        "    plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    if show:\n",
        "        from IPython.display import display\n",
        "        display(fig)\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "    # 6) 重要メトリクス（モニタリング用）\n",
        "    stats = {}\n",
        "    if y_batch is not None:\n",
        "        y = tf.convert_to_tensor(y_batch, dtype=tf.float32).numpy()[0, ..., 0]\n",
        "        w = np.minimum(24.0, 1.0 + y)\n",
        "        stats[\"L1_w_y-x_adv\"] = float(np.mean(w * np.abs(y - x_adv)))\n",
        "        stats[\"L1_w_y-x_evo\"] = float(np.mean(w * np.abs(y - x_evo)))\n",
        "    stats[\"flow_mean_mag(px)\"] = float(np.mean(np.sqrt(vy**2 + vx**2)))\n",
        "    stats[\"source_mean(mm/h)\"] = float(np.mean(np.abs(source)))\n",
        "    stats[\"neg_before_relu(%)\"] = float(100.0 * np.mean((x_adv + source) < 0.0))\n",
        "\n",
        "    if print_stats:\n",
        "        print(\"Metrics:\", stats)\n",
        "\n",
        "    return {\"x_prev\": x_prev, \"x_adv\": x_adv, \"x_evo\": x_evo,\n",
        "            \"source\": source, \"flow_vy\": vy, \"flow_vx\": vx, \"stats\": stats}"
      ],
      "metadata": {
        "id": "0BFko7dqD0tP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowSourceVizCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, val_sample, quiver_step=8, tag_prefix=\"evo\"):\n",
        "        \"\"\"\n",
        "        val_sample: (x_val, y_val)  いずれも numpy (B,T,H,W,1) / (B,H,W,1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sample_x, self.sample_y = val_sample\n",
        "        self.quiver_step = quiver_step\n",
        "        self.tag_prefix = tag_prefix\n",
        "\n",
        "        # JMA カラーを RGB 3ch (0-1) へ\n",
        "        self.jma_colors = np.array([[int(h[i:i+2],16)/255. for i in (1,3,5)]\n",
        "                                    for h in JMA_COLORS], dtype=np.float32)\n",
        "\n",
        "    def _to_rgb(self, mmhr_2d):\n",
        "        mmhr_2d = np.clip(mmhr_2d, 0, 1000)\n",
        "        idx = np.digitize(mmhr_2d, JMA_BOUNDS) - 1\n",
        "        idx = np.clip(idx, 0, len(self.jma_colors)-1)\n",
        "        rgb = self.jma_colors[idx]                  # (H,W,3)\n",
        "        return rgb.transpose(2,0,1)[None,...]       # (1,3,H,W)\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.writer = tensorboardX.SummaryWriter()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        x = self.sample_x[:1]   # 1サンプルだけ\n",
        "        y = self.sample_y[:1] if self.sample_y is not None else None\n",
        "\n",
        "        # 予測と中間生成物\n",
        "        out = visualize_flow_source(self.model, x, y,\n",
        "                                    quiver_step=self.quiver_step,\n",
        "                                    figsize=(12,6),\n",
        "                                    show=False,\n",
        "                                    print_stats=False)\n",
        "        x_prev, x_adv, x_evo, source = out[\"x_prev\"], out[\"x_adv\"], out[\"x_evo\"], out[\"source\"]\n",
        "        vy, vx = out[\"flow_vy\"], out[\"flow_vx\"]\n",
        "\n",
        "        # 画像（RGB）をTBへ\n",
        "        for name, arr in [(\"x_prev\", x_prev), (\"x_adv\", x_adv), (\"x_evo\", x_evo), (\"source\", source)]:\n",
        "            rgb = self._to_rgb(arr.astype(np.float32))\n",
        "            self.writer.add_images(f\"{self.tag_prefix}/{name}\", rgb, epoch)\n",
        "\n",
        "        # flow magnitude をグレースケールで\n",
        "        mag = np.sqrt(vy**2 + vx**2)\n",
        "        mag_norm = (mag / (np.max(mag) + 1e-6)).astype(np.float32)[None,None,...]  # (1,1,H,W)\n",
        "        self.writer.add_images(f\"{self.tag_prefix}/flow_mag_norm\", mag_norm, epoch)\n",
        "\n",
        "        # スカラーメトリクス\n",
        "        for k, v in out[\"stats\"].items():\n",
        "            self.writer.add_scalar(f\"{self.tag_prefix}/{k}\", v, epoch)"
      ],
      "metadata": {
        "id": "ttw0KtiDD2zm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習"
      ],
      "metadata": {
        "id": "RYFz3fniry1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorBoardXCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, val_sample):\n",
        "        super().__init__()\n",
        "        # --- JMA カラーマップ ---\n",
        "        cmap_hex = [\"#f2f2ff\",\"#a0d2ff\",\"#218cff\",\"#0000ff\",\n",
        "                    \"#faf500\",\"#ff9900\",\"#ff2100\",\"#c800c8\"]\n",
        "        self.jma_colors = np.array([[int(h[i:i+2],16)/255. for i in (1,3,5)]\n",
        "                                    for h in cmap_hex], dtype=np.float32)\n",
        "        self.color_border = [0,1,5,10,20,30,50,80,1000]\n",
        "\n",
        "        self.sample_x = val_sample  # 1 バッチだけ保持しておく\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.writer = tensorboardX.SummaryWriter()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # ------ 損失を記録 ------\n",
        "        self.writer.add_scalar(\"loss/train\", logs[\"loss\"],     epoch)\n",
        "        self.writer.add_scalar(\"loss/val\",   logs[\"val_loss\"], epoch)\n",
        "\n",
        "        # ------ 推論画像を記録 ------\n",
        "        pred = self.model.predict(self.sample_x, verbose=0)[0]     # (H,W,1)\n",
        "        pred = np.clip(pred[...,0], 0, 1e9)                       # (H,W)\n",
        "\n",
        "        idx  = np.digitize(pred, self.color_border) - 1            # (H,W)\n",
        "        rgb  = self.jma_colors[idx]                                # (H,W,3)\n",
        "        rgb  = rgb.transpose(2,0,1)[None,...]                      # (1,3,H,W)\n",
        "\n",
        "        self.writer.add_images(\"pred\", rgb, epoch)"
      ],
      "metadata": {
        "id": "Ro-P910_qzjU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from typing import Generator, Tuple, Optional\n",
        "\n",
        "def seq_batch_generator(rains: np.ndarray,\n",
        "                       batch_size: int,\n",
        "                       lon_slice: slice,\n",
        "                       lat_slice: slice,\n",
        "                       seq_len: int = 6,\n",
        "                       downsample: int = 5,\n",
        "                       shuffle: bool = True):\n",
        "    \"\"\"\n",
        "    単一のnumpy配列からシーケンスバッチを生成するジェネレータ\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    rains : np.ndarray\n",
        "        形状 (T, H, W) の降水量データ\n",
        "    batch_size : int\n",
        "        バッチサイズ\n",
        "    lon_slice, lat_slice : slice\n",
        "        経度・緯度方向のスライス\n",
        "    seq_len : int\n",
        "        シーケンス長\n",
        "    downsample : int\n",
        "        ダウンサンプリング係数\n",
        "    shuffle : bool\n",
        "        インデックスをシャッフルするかどうか\n",
        "\n",
        "    Yields:\n",
        "    -------\n",
        "    Tuple[np.ndarray, np.ndarray]\n",
        "        (入力データ, ターゲットデータ)のタプル\n",
        "        入力: (batch_size, seq_len, H, W, 1)\n",
        "        ターゲット: (batch_size, H, W, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # データの前処理\n",
        "    data = rains[:, lat_slice, lon_slice]\n",
        "    data = data[:, ::downsample, ::downsample]\n",
        "\n",
        "    T = data.shape[0]\n",
        "\n",
        "    while True:  # 無限ループでエポックを繰り返す\n",
        "        # 有効なインデックスを生成\n",
        "        indices = list(range(0, T - seq_len - 1))\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        # バッチサイズずつ処理\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "\n",
        "            xs, ys = [], []\n",
        "\n",
        "            for t in batch_indices:\n",
        "                # シーケンス窓を取得\n",
        "                window = data[t:t + seq_len + 1]\n",
        "\n",
        "                # 欠測値チェック\n",
        "                if np.isnan(window).any():\n",
        "                    continue\n",
        "\n",
        "                # 入力とターゲットに分割\n",
        "                x_seq = window[:-1][..., None]  # (seq_len, H, W, 1)\n",
        "                y_target = window[-1][..., None]  # (H, W, 1)\n",
        "\n",
        "                xs.append(x_seq)\n",
        "                ys.append(y_target)\n",
        "\n",
        "            if xs:\n",
        "                yield np.array(xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
        "\n",
        "def get_importance_validation_sample_fixed(date_list: List[str],\n",
        "                                         batch_size: int,\n",
        "                                         lon_slice: slice,\n",
        "                                         lat_slice: slice,\n",
        "                                         seq_len: int = 6,\n",
        "                                         downsample: int = 5,\n",
        "                                         max_attempts: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    修正版: Importance Samplingを使った検証サンプルを安全に取得\n",
        "    ジェネレータを使い回さず、専用の取得関数として実装\n",
        "    \"\"\"\n",
        "\n",
        "    from google.colab import userdata\n",
        "    import boto3\n",
        "    import blosc2\n",
        "    import os\n",
        "\n",
        "    # Wasabi Cloud設定\n",
        "    WASABI_ENDPOINT = \"https://s3.ap-northeast-1.wasabisys.com\"\n",
        "    WASABI_ACCESS_KEY = userdata.get('WASABI_ACCESS_KEY')\n",
        "    WASABI_SECRET_KEY = userdata.get('WASABI_SECRET_KEY')\n",
        "    BUCKET_NAME = \"xrain-composite-cx\"\n",
        "\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        endpoint_url=WASABI_ENDPOINT,\n",
        "        aws_access_key_id=WASABI_ACCESS_KEY,\n",
        "        aws_secret_access_key=WASABI_SECRET_KEY,\n",
        "        region_name='ap-northeast-1'\n",
        "    )\n",
        "\n",
        "    def load_single_date(date: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"単一の日付のデータを安全にロード\"\"\"\n",
        "        try:\n",
        "            base_path = \"b2nd\"\n",
        "            year, month = date[0:4], date[4:6]\n",
        "            filename = f\"{base_path}/{year}/{month}/cx-{date}.b2nd\"\n",
        "            temp_filename = f\"/tmp/cx-{date}-sample.b2nd\"\n",
        "\n",
        "            s3_client.download_file(BUCKET_NAME, filename, temp_filename)\n",
        "            rains = blosc2.open(temp_filename)\n",
        "            data = rains[:, lat_slice, lon_slice]\n",
        "            data = data[:, ::downsample, ::downsample]\n",
        "\n",
        "            if os.path.exists(temp_filename):\n",
        "                os.remove(temp_filename)\n",
        "\n",
        "            return data.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load validation sample for {date}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # 最初の日付からデータをロード\n",
        "    sample_date = date_list[0]\n",
        "    data = load_single_date(sample_date)\n",
        "\n",
        "    print(f\"Loaded validation sample from {sample_date}, shape: {data.shape}\")\n",
        "\n",
        "    # クロップ生成と重要度計算\n",
        "    crop_size = (256, 256)\n",
        "    T, H, W = data.shape\n",
        "    crop_h, crop_w = crop_size\n",
        "\n",
        "    best_crop = None\n",
        "    best_score = -1\n",
        "\n",
        "    # 複数の位置でクロップを試行\n",
        "    for attempt in range(max_attempts):\n",
        "        # ランダムな位置を選択\n",
        "        h_start = np.random.randint(0, max(1, H - crop_h))\n",
        "        w_start = np.random.randint(0, max(1, W - crop_w))\n",
        "        h_end = min(h_start + crop_h, H)\n",
        "        w_end = min(w_start + crop_w, W)\n",
        "\n",
        "        crop = data[:, h_start:h_end, w_start:w_end]\n",
        "\n",
        "        if crop.shape[0] <= seq_len:\n",
        "            continue\n",
        "\n",
        "        # 受容確率を計算\n",
        "        score = calculate_acceptance_probability(crop, training_mode=False)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_crop = crop\n",
        "\n",
        "    if best_crop is None:\n",
        "        print(\"Warning: No valid crop found, using first available\")\n",
        "        best_crop = data[:, :crop_h, :crop_w]\n",
        "\n",
        "    # シーケンスを生成\n",
        "    T_crop = best_crop.shape[0]\n",
        "\n",
        "\n",
        "    # 最も降水量が多い時点を中心にシーケンスを選択\n",
        "    max_rain_per_frame = [np.max(best_crop[t]) for t in range(T_crop)]\n",
        "    best_end_idx = np.argmax(max_rain_per_frame)\n",
        "\n",
        "    # シーケンスの開始点を調整\n",
        "    start_idx = max(0, min(best_end_idx - seq_len, T_crop - seq_len - 1))\n",
        "\n",
        "    window = best_crop[start_idx:start_idx + seq_len + 1]\n",
        "\n",
        "    # 入力とターゲットに分割\n",
        "    x_seq = window[:-1][..., None]  # (seq_len, H, W, 1)\n",
        "    y_target = window[-1][..., None]  # (H, W, 1)\n",
        "\n",
        "    # バッチ次元を追加\n",
        "    x_batch = x_seq[np.newaxis]  # (1, seq_len, H, W, 1)\n",
        "    y_batch = y_target[np.newaxis]  # (1, H, W, 1)\n",
        "\n",
        "    print(f\"Generated validation sample with max rain: {np.max(y_target):.2f} mm/h\")\n",
        "\n",
        "    return x_batch.astype(np.float32), y_batch.astype(np.float32)\n",
        "\n",
        "\n",
        "def create_finite_generator(date_list: List[str],\n",
        "                          batch_size: int,\n",
        "                          lon_slice: slice,\n",
        "                          lat_slice: slice,\n",
        "                          seq_len: int = 6,\n",
        "                          downsample: int = 5,\n",
        "                          max_batches_per_epoch: int = 100) -> Generator:\n",
        "    \"\"\"\n",
        "    有限回数で停止するジェネレータ（デバッグ用）\n",
        "    \"\"\"\n",
        "    batch_count = 0\n",
        "\n",
        "    for x_batch, y_batch in importance_sampling_generator(\n",
        "        date_list, batch_size, lon_slice, lat_slice,\n",
        "        seq_len, downsample, buffer_size=2\n",
        "    ):\n",
        "        yield x_batch, y_batch\n",
        "        batch_count += 1\n",
        "\n",
        "        if batch_count >= max_batches_per_epoch:\n",
        "            print(f\"Generator stopped after {batch_count} batches (safety limit)\")\n",
        "            break\n",
        "\n",
        "\n",
        "val_sample_x, val_sample_y = get_importance_validation_sample_fixed(\n",
        "    val_dates, BATCH_SIZE, LON_SLICE, LAT_SLICE, SEQ_LEN, DOWNSAMPLE\n",
        ")\n",
        "\n",
        "# 4. コールバックの更新\n",
        "fs_cb_importance = FlowSourceVizCallback(\n",
        "    (val_sample_x, val_sample_y),\n",
        "    quiver_step=8,\n",
        "    tag_prefix=\"evo_importance\"\n",
        ")\n",
        "\n",
        "print(\"=== Importance Sampling Configuration ===\")\n",
        "print(f\"Original steps per epoch: {multi_steps_per_epoch}\")\n",
        "print(f\"New steps per epoch: {importance_steps_per_epoch}\")\n",
        "print(f\"Original val steps: {multi_val_steps}\")\n",
        "print(f\"New val steps: {importance_val_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qFgtXnQVcqJ",
        "outputId": "bba050f7-3b11-43a5-f841-2284e207d0e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded validation sample from 20240819, shape: (1440, 192, 128)\n",
            "Generated validation sample with max rain: nan mm/h\n",
            "=== Importance Sampling Configuration ===\n",
            "Original steps per epoch: 449\n",
            "New steps per epoch: 3\n",
            "Original val steps: 179\n",
            "New val steps: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 最終的なmodel.fit呼び出し\n",
        "model.fit(\n",
        "    train_gen,  # Importance Samplingジェネレータ\n",
        "    steps_per_epoch=multi_steps_per_epoch,  # 計算されたsteps\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_gen,  # Importance Samplingジェネレータ\n",
        "    validation_steps=val_steps,  # 計算されたsteps\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        keras.callbacks.TerminateOnNaN(),\n",
        "        TensorBoardXCallback(val_sample_x[:1]),  # 検証サンプル\n",
        "        fs_cb_importance,  # コールバック\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP3PAk4Tr3mp",
        "outputId": "1bf6794d-f1a5-4269-93d4-e6bc6ad32410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data for dates ['20240809', '20240810'], combined shape: (2880, 192, 128)\n",
            "Loaded data for dates ['20240810'], combined shape: (1440, 192, 128)\n",
            "Epoch 1/200\n",
            "\u001b[1m  2/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:46\u001b[0m 507ms/step - accum_loss: 1.9274 - loss: 1.9274 - mae: 0.0615 - motion_reg: 4.6471e-04 Loaded data for dates ['20240809', '20240810'], combined shape: (2880, 192, 128)\n",
            "\u001b[1m  3/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30:42\u001b[0m 20s/step - accum_loss: 2.0517 - loss: 2.0517 - mae: 0.0654 - motion_reg: 6.0858e-04Loaded data for dates ['20240810'], combined shape: (1440, 192, 128)\n",
            "\u001b[1m  4/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20:33\u001b[0m 19s/step - accum_loss: 2.0046 - loss: 2.0046 - mae: 0.0641 - motion_reg: 7.1675e-04Loaded data for dates ['20240809', '20240810'], combined shape: (2880, 192, 128)\n",
            "\u001b[1m  5/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:38:17\u001b[0m 21s/step - accum_loss: 1.9885 - loss: 1.9885 - mae: 0.0638 - motion_reg: 8.3583e-04Loaded data for dates ['20240810'], combined shape: (1440, 192, 128)\n",
            "\u001b[1m  6/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:35:07\u001b[0m 21s/step - accum_loss: 1.9394 - loss: 1.9394 - mae: 0.0632 - motion_reg: 9.5314e-04Loaded data for dates ['20240809', '20240810'], combined shape: (2880, 192, 128)\n",
            "\u001b[1m  7/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:58:14\u001b[0m 24s/step - accum_loss: 1.9518 - loss: 1.9518 - mae: 0.0644 - motion_reg: 0.0011    Loaded data for dates ['20240810'], combined shape: (1440, 192, 128)\n",
            "\u001b[1m  8/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:51:19\u001b[0m 23s/step - accum_loss: 1.9353 - loss: 1.9353 - mae: 0.0645 - motion_reg: 0.0012Loaded data for dates ['20240809', '20240810'], combined shape: (2880, 192, 128)\n",
            "\u001b[1m  9/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:02:03\u001b[0m 25s/step - accum_loss: 1.9510 - loss: 1.9510 - mae: 0.0652 - motion_reg: 0.0014"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習結果の表示"
      ],
      "metadata": {
        "id": "tTBQhgstBqEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "plt.ioff()  # jupyter notebookで自動的にグラフが表示されないようにする\n",
        "\n",
        "jma_colors = [\n",
        "    \"#f2f2ff\",\n",
        "    \"#a0d2ff\",\n",
        "    \"#218cff\",\n",
        "    \"#0000ff\",\n",
        "    \"#faf500\",\n",
        "    \"#ff9900\",\n",
        "    \"#ff2100\",\n",
        "    \"#c800c8\",\n",
        "]\n",
        "jma_bounds = [0, 1, 5, 10, 20, 30, 50, 80, 1000]\n",
        "jma_cmap = mcolors.ListedColormap(jma_colors)\n",
        "jma_norm = mcolors.BoundaryNorm(jma_bounds, jma_cmap.N)\n",
        "jma_cmap.set_bad(\"#ffffff\")\n"
      ],
      "metadata": {
        "id": "T7l942Fet_dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.figure\n",
        "\n",
        "\n",
        "# 2つの雨量データを並べて表示する関数\n",
        "def plot2(fig: matplotlib.figure.Figure, data0, data1):\n",
        "    ax0 = fig.add_subplot(1, 2, 1)\n",
        "    ax1 = fig.add_subplot(1, 2, 2)\n",
        "\n",
        "    # 配色用の ScalarMappable を一度だけ生成\n",
        "    sm = plt.cm.ScalarMappable(cmap=jma_cmap, norm=jma_norm)\n",
        "    sm.set_array([])\n",
        "\n",
        "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
        "    fig.colorbar(sm, cax=cax, orientation=\"vertical\",\n",
        "                 boundaries=jma_bounds, spacing=\"uniform\",\n",
        "                 ticks=[1,5,10,20,30,50,80]).set_label(\"mm/h\")\n",
        "\n",
        "    ax0.imshow(data0, cmap=jma_cmap, norm=jma_norm)\n",
        "    ax1.imshow(data1, cmap=jma_cmap, norm=jma_norm)\n",
        "    ax0.set_title(\"input\")\n",
        "    ax1.set_title(\"output\")"
      ],
      "metadata": {
        "id": "PSqDr4nnBxQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_input_seq(rains: np.ndarray, t: int, seq_len: int = SEQ_LEN):\n",
        "    \"\"\"\n",
        "    t-seq_len … t-1 の窓を (1,T,H,W,1) で返す\n",
        "    行 = 緯度(LAT_SLICE) / 列 = 経度(LON_SLICE)\n",
        "    \"\"\"\n",
        "    assert t >= seq_len, \"t は SEQ_LEN 以上にしてください\"\n",
        "\n",
        "    seq = rains[t-seq_len : t,\n",
        "                LAT_SLICE,   # ← 行方向 = 緯度\n",
        "                LON_SLICE]   # ← 列方向 = 経度\n",
        "    seq = seq[:, ::DOWNSAMPLE, ::DOWNSAMPLE]   # 10 倍間引き\n",
        "    seq = seq[..., None]                       # (T,H,W,1)\n",
        "    return seq[np.newaxis].astype(\"float32\")"
      ],
      "metadata": {
        "id": "ZKLwyZEIBzPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_in = make_input_seq(val_rains, SEQ_LEN)\n",
        "print(\"x_in shape :\", x_in.shape)   # → (1, 6, 192, 128, 1)\n",
        "\n",
        "y_pred = model(x_in, training=False).numpy()[0, ..., 0]   # (192,128)\n",
        "print(\"y_pred shape:\", y_pred.shape)"
      ],
      "metadata": {
        "id": "WGhrEuglB-66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = SEQ_LEN + 0                              # 表示したいフレーム番号\n",
        "x_in   = make_input_seq(val_rains, i)        # (1,T,H,W,1)\n",
        "y_pred = model(x_in, training=False).numpy()[0, ..., 0]   # (H,W)\n",
        "\n",
        "y_true = val_rains[i,\n",
        "                   LAT_SLICE,  # 行 = 緯度\n",
        "                   LON_SLICE]  # 列 = 経度\n",
        "y_true = y_true[::DOWNSAMPLE, ::DOWNSAMPLE]\n",
        "\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "plot2(fig, y_true, y_pred)   # True → Pred を並べて表示\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sKi7f9p0PBck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "init_i      = SEQ_LEN\n",
        "predict_num = val_rains.shape[0] - init_i   # val_rains 全体から SEQ_LEN 枚を引いた数\n",
        "# predict_num = 2\n",
        "\n",
        "# ── 2) 結果を格納する配列を確保 ───────────────────────────────\n",
        "H = math.ceil((LAT_SLICE.stop - LAT_SLICE.start) / DOWNSAMPLE)\n",
        "W = math.ceil((LON_SLICE.stop - LON_SLICE.start) / DOWNSAMPLE)\n",
        "predict_rain = np.zeros((predict_num, H, W), dtype=np.float32)\n",
        "\n",
        "# ── 3) val_rains に対してループ推論 ───────────────────────────\n",
        "for i in tqdm(range(predict_num), desc=\"Predicting on val_rains\"):\n",
        "    t = init_i + i\n",
        "    x_in   = make_input_seq(val_rains, t)                           # (1,SEQ_LEN,H,W,1)\n",
        "    y_pred = model(x_in, training=False).numpy()[0, ..., 0]         # → (H,W)\n",
        "    predict_rain[i] = y_pred"
      ],
      "metadata": {
        "id": "3MBGKcJzaTA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "def update_plot(i):\n",
        "    fig.clear()\n",
        "    # 真値を切り出し＆ダウンサンプリング\n",
        "    true_frame = val_rains[i + init_i,\n",
        "                           LAT_SLICE, LON_SLICE][::DOWNSAMPLE, ::DOWNSAMPLE]\n",
        "    pred_frame = predict_rain[i]  # (H,W)\n",
        "    plot2(fig, true_frame, pred_frame)\n",
        "    return []\n",
        "\n",
        "ani = animation.FuncAnimation(\n",
        "    fig,\n",
        "    update_plot,\n",
        "    frames=range(predict_num),\n",
        "    interval=200,   # ミリ秒\n",
        "    blit=False\n",
        ")\n",
        "\n",
        "# 保存（MP4）\n",
        "writer = animation.FFMpegWriter(fps=10, codec='libx264')\n",
        "ani.save(\"predict_val.mp4\", writer=writer, dpi=200)\n",
        "\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "y4tSjMqtPwur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, Video\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('predict_val.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<video width=\"800\" controls>\n",
        "  <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "DtwloS9SCNjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自己回帰"
      ],
      "metadata": {
        "id": "u98cvGf4CWGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# ---- 開始フレーム（0-based）----\n",
        "START_T = 1070  # ← ここを変更（1-basedなら 1069 にする）\n",
        "\n",
        "T_total = val_rains.shape[0]\n",
        "assert START_T >= SEQ_LEN, \"START_T は SEQ_LEN 以上にしてください\"\n",
        "\n",
        "# ---- 予測枚数：START_T から最後まで ----\n",
        "predict_num = T_total - START_T\n",
        "# predict_num = 2\n",
        "\n",
        "# ---- 履歴を START_T の直前 SEQ_LEN 枚の真値で初期化 ----\n",
        "history = deque(maxlen=SEQ_LEN)\n",
        "for t in range(START_T - SEQ_LEN, START_T):\n",
        "    frame = val_rains[t, LAT_SLICE, LON_SLICE][::DOWNSAMPLE, ::DOWNSAMPLE]\n",
        "    history.append(frame.astype(\"float32\"))\n",
        "\n",
        "# ---- 推論ループ ----\n",
        "predict_rain = np.empty((predict_num, H, W), dtype=\"float32\")\n",
        "\n",
        "for i in range(predict_num):\n",
        "    # 履歴 → (1,SEQ_LEN,H,W,1)\n",
        "    x = np.stack(history, axis=0)[np.newaxis, ..., np.newaxis]\n",
        "\n",
        "    # 1枚先（時刻 t = START_T + i）を予測\n",
        "    y_hat = model(x, training=False).numpy()[0, ..., 0]  # (H,W)\n",
        "\n",
        "    # 保存 & 履歴更新（自己回帰）\n",
        "    predict_rain[i] = y_hat\n",
        "    history.append(y_hat)\n",
        "\n",
        "print(\"START_T =\", START_T)\n",
        "print(\"predict_rain.shape =\", predict_rain.shape)  # → (T_total - START_T, H, W)"
      ],
      "metadata": {
        "id": "xBBClxf8CaTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True)\n",
        "\n",
        "# カラーバー（1回だけ作成）\n",
        "sm = plt.cm.ScalarMappable(cmap=jma_cmap, norm=jma_norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax.ravel().tolist(), orientation=\"vertical\", fraction=0.046, pad=0.02)\n",
        "cbar.set_label(\"mm/h\")\n",
        "\n",
        "def update_plot(i):\n",
        "    ax[0].cla()\n",
        "    ax[1].cla()\n",
        "\n",
        "    # 左：真値（時刻 START_T+i）\n",
        "    true_frame = val_rains[START_T + i, LAT_SLICE, LON_SLICE][::DOWNSAMPLE, ::DOWNSAMPLE]\n",
        "    im0 = ax[0].imshow(true_frame, vmin=0, vmax=SCALE_FACTOR, cmap=\"Blues\")\n",
        "    ax[0].set_title(f\"Ground Truth (t={START_T + i})\")\n",
        "\n",
        "    # 右：自己回帰予測（i 番目）\n",
        "    pred_frame = predict_rain[i]\n",
        "    im1 = ax[1].imshow(pred_frame, vmin=0, vmax=SCALE_FACTOR, cmap=\"Blues\")\n",
        "    ax[1].set_title(f\"Prediction (t={START_T + i})\")\n",
        "\n",
        "    return im0, im1\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "ani = animation.FuncAnimation(\n",
        "    fig, update_plot, frames=range(predict_num), interval=200\n",
        ")\n",
        "\n",
        "writer = animation.FFMpegWriter(fps=10, codec=\"libx264\")\n",
        "ani.save(\"predict_val_self.mp4\", writer=writer, dpi=200)\n",
        "\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "0MYht1LdOY6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp4 = open('predict_val_self.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<video width=\"800\" controls>\n",
        "  <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "FCj-gmROCiyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "8S67vEDuCnOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aYpxCT7EVbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}